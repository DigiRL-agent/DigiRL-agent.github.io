<!DOCTYPE html>
<html>

<head>
    <title>DigiRL</title>
    <link rel="icon" href="website/images/icon/icon.png" type="image/icon type">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->

    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis2.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');
    </script>

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            <img src="website/images/icon/icon.png" alt="logo" width="40" height="40" />
                            DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://www.jackgethome.com/">Hao Bai</a><sup>1,2*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://https://yifeizhou02.github.io">Yifei Zhou</a><sup>1*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=sMEFwf8AAAAJ&hl=en">Mert Cemri</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.jiayipan.me/">Jiayi Pan</a><sup>1</sup>,
                            </span>
                            <br>
                            <span class="author-block">
                                <a href="https://www.alanesuhr.com/">Alane Suhr</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a><sup>3</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup> UC Berkeley,</span>
                            <span class="author-block"><sup>2</sup> UIUC,</span>
                            <span class="author-block"><sup>3</sup> Google DeepMind</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2309.10691" class="btn btn-outline-dark"
                                        role="button">&#128221;
                                        Paper</a> &nbsp;&nbsp;

                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/xingyaoww/mint-bench" class="btn btn-outline-dark"
                                        role="button">&#128187;
                                        Code</a> &nbsp;&nbsp;

                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">

                                    <a href="https://github.com/xingyaoww/mint-bench/blob/main/docs/DATA.md"
                                        class="btn btn-outline-dark" role="button">&#128194;
                                        Data</a>
                            </div>
                        </div>

                        <!-- <h2 class="subtitle" style="text-align: left;">
                            <b>MINT benchmark</b> measures LLMs' ability to solve tasks with multi-turn interactions
                            by
                            (1) using tools and (2) leveraging natural language feedback.
                        </h2> -->
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" id="abstract">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="content has-text-justified">
                    <div class="iframe-container">
                        <iframe src="website/videos/demo.mp4" class="myIframe" frameborder="0" allow="autoplay; fullscreen"></iframe>
                    </div>
                </div>
            </div>
        </div>
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Training corpuses for vision language models typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short when controlling <b>real</b> GUIs due to their failure to deal with real world <b>stochasticity</b> not captured in static observational data.  
                            <br>  
                            This paper introduces a novel autonomous RL approach, called <b>DigiRL</b>, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offline-to-online RL. To do this, we build a <b>scalable</b> and <b>parallelizable</b> Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. 
                            <br>
                            We demonstrate the effectiveness of <b>DigiRL</b> using the Android-in-the-Wild (AitW) dataset, where our 1.5B VLM trained with RL achieves a <b>49.5% absolute improvement</b> -- from 17.7% to 67.2% success rate -- over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent trained with AitW data (14.4%), but also the prior best autonomous RL approach based on filtered behavior cloning (57.8%), thereby <b>establishing a new state-of-the-art</b> for digital agents for in-the-wild device control.    
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
        <div class="container is-max-desktop">
            <section class="section" id="abstract">
                <div class="container is-max-desktop">
                    <!-- Abstract. -->
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">Demo</h2>
                            <div class="content has-text-justified">
                                <h4>AitW General Demo: Search for some good Italian restaurants.</h4>
                                <div class="iframe-container">
                                    <iframe src="website/videos/general.mp4" class="myIframe" frameborder="0" allow="autoplay; fullscreen; loop"></iframe>
                                </div>
                            </div>
                            <div class="content has-text-justified">
                                <h4>AitW Web Shopping Demo: Go to newegg.com, and search for "Alienware Aurora".</h4>
                                <div class="iframe-container">
                                    <iframe src="website/videos/webshop.mp4" class="myIframe" frameborder="0" allow="autoplay; fullscreen; loop"></iframe>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">

                <h2 class="subtitle">
                    <b>DigiRL</b> solves open-ended realistic Android tasks with an novel online reinforcement learning algorithm with autonomous VLM evaluator.
                </h2>

                <ul class="nav nav-tabs" id="myTab" role="tablist">
                    <li class="nav-item" role="presentation">
                        <button class="nav-link active" id="main-results-tab" data-bs-toggle="tab"
                            data-bs-target="#benchmark-table-content" type="button" role="tab"
                            aria-controls="main-results-tab" aria-selected="true">Success Rates</button>
                    </li>
                    <!-- <li class="nav-item" role="presentation">
                        <button class="nav-link" id="eurus-code-table-tab" data-bs-toggle="tab"
                            data-bs-target="#eurus-code-table-content" type="button" role="tab"
                            aria-controls="eurus-code-table-tab" aria-selected="false">Code (Eurus subset)</button>
                    </li>
                    <li class="nav-item" role="presentation">
                        <button class="nav-link" id="eurus-math-table-tab" data-bs-toggle="tab"
                            data-bs-target="#eurus-math-table-content" type="button" role="tab"
                            aria-controls="eurus-math-table-tab" aria-selected="false">
                            Math (Eurus subset)</button>
                    </li> -->
                </ul>

                <div class="tab-content" id="myTabContent">
                    <div class="tab-pane fade show active" id="benchmark-table-content" role="tabpanel"
                        aria-labelledby="benchmark-table-content">

                        <p class="mt-2 px-2">
                            This table contains the success rate across all approaches measured in the
                            <a href="https://arxiv.org/abs/2309.10691">DigiRL paper</a>. It includes performance on
                            two subsets: AitW General and AitW Web Shopping. The codename of GPT-4V we use is <i>gpt-4-vision-preview</i>
                            and the codename of Gemini-1.5-Pro is <i>gemini-1.5-pro-latest</i>.
                        </p>

                        <div id="benchmark-table"></div>
                    </div>
                    <div class="tab-pane fade" id="eurus-code-table-content" role="tabpanel"
                        aria-labelledby="eurus-code-table-content">

                        <p class="mt-2 px-2">
                            This code subset follows the <a href="https://arxiv.org/abs/2404.02078">Eurus
                                paper</a> and contains MBPP and HumanEval.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="interaction-framework">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">DigiRL: Autonomous RL for Building a Strong Device-Control Agent</h2>

                        <h3> Why RL over the alternatives? </h3>
                        <ul>
                            <li>LLM Agent data such as device-control actions is poorly represented in the pre-training corpus of <b>Off-the-shelf proprietary VLMs</b> such as GPT4V and Gemini-1.5-Pro.</li>
                            <li><b>Supervised Fine-Tuning</b> 1) requires a large amount of human demonstration data and 2) cannot recover from degrading model performance when real websites/applications have changed. As shown in the plot below, a frozen good policy trained with prior data experiences a gradual drop in performance as the websites change over time, while the DigiRL policy constantly updates with fresh autonomous data can maintain a stable performance.</li>
                        </ul>
                        
                        <div class="chart-container" id="chart-k2" style="display:block;margin:0 auto;">
                            <canvas id="chart-sr-vs-k2"></canvas>
                        </div>
                        <h3> What are we using RL for?</h3>
                        DigiRL consists of two steps:
                        <ul>
                            <li>First, we use <b>Offline RL</b> to make the most out of a potentially sub-optimal existing offline dataset.</li>
                            <li>Then, we use <b>Offline-to-Online RL</b> to encourage the agent to learn from its own trials and errors.</li>
                        </ul>
                        DigiRL identifies the most simple yet effective RL design choices for device-control agent problems. Our RL algorithmic framework automatically achieves the following advantages compared to state-of-the-art alternatives such as rejection sampling (or Filtered Behavior Cloning):
                        <ul>
                            <li>We makes use of an <b>instruction-level value function</b> to implicitly construct an automatic curriculum that prioritizes on the tasks most informative to the agent.</li>
                            <li>We makes use of a <b>step-level value function</b> to pick out the advantageous actions (actions that mark progress towards the goal) in a trajectory while leaving the noisy actions (actions that do not contribute to the goal).</li>
                        </ul>
                        Please check out our paper for more details of our algorithm!

                        <div style="text-align:center;">
                            <img src="website/images/algo.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <br>
                        </div>
                        <h3>Learning Curves</h3>
                        <div class="text-justify" id="tool-augmented">
                            In addition to the convergence performance reported in the paper, we also present the sample complexity comparison of DigiRL against the state-of-the-art alternative Filtered Behavior Cloning (or rejection sampling). We find that DigiRL not only converges to a superior performance, but also learns more efficiently.
                            <br>
                            <div class="btn-group main-curve" role="group">
                                <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled
                                    id="visualize-sr-vs-k-scale-with-model-size-llama2-base-disabled">Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="visualize-sr-vs-k-scale-with-model-size-llama2-base">AitW General</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="visualize-sr-vs-k-scale-with-model-size-llama2-rlhf">AitW Web Shopping</button>
                            </div>
                        </div>
                        <div class="chart-container" id="chart-k" style="display:block;margin:0 auto;">
                            <canvas id="chart-sr-vs-k"></canvas>
                        </div>

                    </div>
                </div>
                <!--/ Visual Effects. -->

            </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <h3 class="title is-3">Evaluation</h3>
                        <p>
                            Our main results are autonomously evaluated with Gemini-1.5-Pro. We also manually evaluate on some subsets and finds that the autonomous evaluation results highly align with manual evaluations with an average difference less than 3%:
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center feedback-provider-sort-by-selector"
                                data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedback-gain">
                                    AitW General
                                </button>

                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedback-provider-perf">
                                    AitW Web Shopping
                                </button>
                            </div>
                        </div>

                        <div class="chart-container" id="chart-feedback-p" style="display:block;margin:0 auto;">
                            <canvas id="chart-feedback-provider"></canvas>
                        </div>

                        <h3>Failure Mode Analysis</h3>

                        <p>
                            While all the types of failure modes benefit from offline and offline-to-online RL training, the most consistent and significant reduction is for the failure mode of failing to recover from mistakes. By training on autonomously-collected rollouts, our agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training.
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="avg_micro">AitW General</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="reasoning">AitW Webshop</button>
                            </div>


                            <!-- <div class="btn-group btn-group-toggle text-center sort-by-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Failure:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedbacksr">Fail to recover from mistakes</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-nofeedbacksr">Get stuck midway</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedbackdelta">Arrive at wrong goal</button>
                            </div> -->
                        </div>

                        <div class="chart-container" id="chart-feedback" style="position:relative;margin:0 auto;">
                            <canvas id="chart-sr-w-feedback" style="max-height: 100%;"></canvas>
                        </div>

                    </div>
                </div>
            </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">
            <div class="columns is-full-width">
                <div class="column">
                    <div class="content">
    <h4>Re the Icon <img src="website/images/icon/icon.png" alt="logo" width="40" height="40" /></h4>
    <p>
        <b>Infinity:</b> Our environment is open-ended, which can be easily generalized to open-ended tasks sets with our open-ended evaluator.
        <br>
        <b>Loop:</b> We use online reinforcement learning, which is closed-loop.
    </p>
    </div>
    </div>
    </div>
    </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{bai2024digirl,
title={DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning},
author={Bai, Hao and Zhou, Yifei and Pan, Jiayi and Cemri, Mert and Suhr, Alane and Levine, Sergey and Kumar, Aviral}
year={2024},
eprint={2405.10292},
archivePrefix={arXiv},
primaryClass={cs.AI}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a
                        href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
                </div>
            </div>
        </div>
    </footer>

</body>


</html>
